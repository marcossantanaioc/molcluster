# AUTOGENERATED! DO NOT EDIT! File to edit: ../../dimensionality_reduction.ipynb.

# %% auto 0
__all__ = ['BaseTransform', 'UMAPTransform', 'PCATransform']

# %% ../../dimensionality_reduction.ipynb 3
from collections import defaultdict
from fastcore.basics import *
from fastcore.foundation import *
from fastcore.meta import *
from typing import Collection, List, Tuple
from umap import UMAP
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %% ../../dimensionality_reduction.ipynb 5
class BaseTransform:
    """Base class to perform dimensionality reduction on a dataset. """
    
    def __init__(self):
        pass

    @property
    def reducer(self):
        return self._reducer
    
    @reducer.setter
    def reducer(self, i):
        self._reducer = i

class UMAPTransform(BaseTransform):
    """Calculate UMAP embeddings"""
    
    def __init__(self, dataset : np.array):
        self.dataset = dataset
            
    def reduce(self, n_neighbors:int=30, min_dist:float=0.5, **kwargs):
        """Performs dimensionality reduction on a dataset using UMAP
        
        Parameters:
        
        n_neighbors: float (optional, default=30)
            The size of local neighborhood (in terms of number of neighboring
            sample points) used for manifold approximation. Larger values
            result in more global views of the manifold, while smaller
            values result in more local data being preserved. In general
            values should be in the range 2 to 100.
   
        min_dist: int (optional, default=0.5)
            The effective minimum distance between embedded points. Smaller values
            will result in a more clustered/clumped embedding where nearby points
            on the manifold are drawn closer together, while larger values will
            result on a more even dispersal of points. The value should be set
            relative to the ``spread`` value, which determines the scale at which
            embedded points will be spread out.
            
            Keyword arguments:
        
                See UMAP documentation for the complete list of arguments (https://umap-learn.readthedocs.io/en/latest/parameters.html)        
        """

        
        reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, **kwargs)
        embeddings = reducer.fit_transform(self.dataset)
        
        self._reducer = reducer
        return embeddings

class PCATransform(BaseTransform):
    """Calculate UMAP embeddings"""
    
    def __init__(self, dataset : np.array):
        self.dataset = dataset
            
    def reduce(self, n_components=None, **kwargs):
        """Performs dimensionality reduction on a dataset using Principal Component Analysis (PCA)
        
        Parameters:
        
            n_components : int, float or 'mle', default=None
                Number of components to keep.
                if n_components is not set all components are kept::
                    n_components == min(n_samples, n_features)
                If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's
                MLE is used to guess the dimension. Use of ``n_components == 'mle'``
                will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.
                If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the
                number of components such that the amount of variance that needs to be
                explained is greater than the percentage specified by n_components.
                If ``svd_solver == 'arpack'``, the number of components must be
                strictly less than the minimum of n_features and n_samples.
                Hence, the None case results in:
                
            n_components == min(n_samples, n_features) - 1
            
            Keyword arguments:
        
                See PCA documentation for the complete list of arguments (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)        
        """

        
        reducer = PCA(n_components=n_components, **kwargs)
        embeddings = reducer.fit_transform(self.dataset)
        
        self._reducer = reducer
        return embeddings

